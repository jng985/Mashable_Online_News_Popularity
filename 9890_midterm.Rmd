---
title: "9890 Midterm"
author: "Jonathan Ng"
date: "2/23/2020"
output: pdf_document
---

**1.** Suppose we observe a vector $y \in \mathbb{R}^n$ of observations from the model,
$$ y=X\beta^* + \epsilon, $$
where $X \in \mathbb{R}^{n\times{p}}$ s a fixed matrix of predictor variables, $\beta^* \in \mathbb{R}^{p}$ is the true unknown coefficient vector that we would like to learn, and $\epsilon \in \mathbb{R}^n$ is a random error vector withidentically, independently distributed entries, with 
$$\mathbb{E}[\epsilon] = 0, \ Cov(\epsilon)=\sigma^2I$$
In other words, we can say $$y_i = x_i^T \beta^* + \epsilon_i,$$
for $i = 1, ..., n$, $x_i^T$ is the ith row of $X$, $\mathbb{E} [\epsilon_i] = 0$ and $Var[\epsilon_i] = \sigma^2$.


**a.** Ridge regression is a modified version of least squares, specially useful for $p > n$ where no unique least squares estimate exist. Ridge regression solves a penalized least squares problem,
$$\hat\beta_{ridge} = \arg\min_{\beta} \Vert y - X\beta \Vert_2^2 + \lambda\Vert\beta\Vert_2^2,$$ 
where $\lambda > 0$ is a fixed constant. Show that $\hat\beta_{ridge}$ is simply the vector of linear regression coefficients from regressing the response $\tilde y = \begin{bmatrix} y \\ 0 \end{bmatrix} \in \mathbb{R}^{n+p}$ onto the predictor matrix $\tilde X = \begin{bmatrix}X \\ \sqrt\lambda I \end{bmatrix} \in \mathbb{R}^{(n+p)\times p}$, where $0 \in \mathbb{R}^p$, and $I \in \mathbb{R}^{p \times p}$ is the identity matrix.

$$
\tilde X =
\begin{bmatrix} \\
    X_{1,1} & X_{1,2} & X_{1,3} & \dots  & X_{1,p} \\
    X_{2,1} & X_{2,2} & X_{2,3} & \dots  & X_{2,p} \\
    X_{3,1} & X_{3,2} & X_{3,3} & \dots  & X_{3,p} \\
    \vdots & \vdots & \vdots & \dots & \vdots \\
    X_{n,1} & X_{n,2} & X_{n,3} & \dots  & X_{n,p} \\
    \sqrt\lambda_{n+1,1} & 0 & 0 & \dots  & 0 \\
    0 & \sqrt\lambda_{n+2,2} & 0 & \dots  & 0 \\
    0  & 0 &  \sqrt\lambda_{n+3,3} & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & \sqrt\lambda_{n+p,p}
\end{bmatrix}, 
\tilde y = \begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \\ 0_{n+1} \\ 0_{n+2}  \\ 0_{n+3} \\ \vdots \\ 0_{n+p} \end{bmatrix}\\
$$


Regressing the $\tilde y$ on $\tilde X$:
$$\tilde X \hat\beta_{ridge} = \tilde y$$
$$\tilde X^T \tilde X \hat\beta_{ridge} = \tilde X^T \tilde y $$
$$\hat\beta_{ridge} = (\tilde X^T \tilde X)^{-1} \ \tilde X^T \tilde y$$
$$\hat\beta_{ridge} = (\begin{bmatrix}X \\ \sqrt\lambda I \end{bmatrix}^T \begin{bmatrix}X \\ \sqrt\lambda I \end{bmatrix})^{-1} \begin{bmatrix}X \\ \sqrt\lambda I \end{bmatrix}^T \begin{bmatrix} y \\ 0 \end{bmatrix}$$
$$\hat\beta_{ridge} = (X^TX + \lambda I)^{-1}X^Ty$$
This yields the same result for $\hat\beta_{ridge}$ as taking the gradient with respect to $\beta$.
$$\hat\beta_{ridge} = \arg\min_{\beta} \Vert y - X\beta \Vert_2^2 + \lambda\Vert\beta\Vert_2^2 $$
$$L(\beta_{ridge}) = \Vert y - X\beta \Vert_2^2 + \lambda\Vert\beta\Vert_2^2 $$
$$\nabla \hat\beta_{ridge} = -2X^T(y-X\beta) \ + \ 2\lambda I\beta$$
$$\nabla_\beta \hat\beta_{ridge} = -2X^Ty + 2 (X^TX + \lambda I) \beta = 0 $$
$$(X^TX + \lambda I) \hat\beta_{ridge} = X^Ty $$
$$\hat\beta_{ridge} = (X^TX + \lambda I)^{-1} X^T y$$

**b.** Show that the matrix $\tilde X$ always has full column-rank, i.e. its columns are always linearly independent, regardless of the columns of $X$. Hence argue that the ridge regression estimate is always unique, for any matrix predictors $X$.


From part a, $\hat\beta_{ridge} = (\tilde X^T \tilde X)^{-1} \tilde X^T \tilde y$, we see that $\tilde X^T \tilde X$ is invertibile. Therefore, the columns of $\tilde X$ must be linearly independent.

Upon further inspection of $\tilde X$ shown below, it can be seen that $\tilde X$ has full column rank regardless of the columns of $X$.

$$\tilde X =
\begin{bmatrix} \\
    X_{1,1} & X_{1,2} & X_{1,3} & \dots  & X_{1,p} \\
    X_{2,1} & X_{2,2} & X_{2,3} & \dots  & X_{2,p} \\
    X_{3,1} & X_{3,2} & X_{3,3} & \dots  & X_{3,p} \\
    \vdots & \vdots & \vdots & \dots & \vdots \\
    X_{n,1} & X_{n,2} & X_{n,3} & \dots  & X_{n,p} \\
    \sqrt\lambda_{n+1,1} & 0 & 0 & \dots  & 0 \\
    0 & \sqrt\lambda_{n+2,2} & 0 & \dots  & 0 \\
    0  & 0 &  \sqrt\lambda_{n+3,3} & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & \sqrt\lambda_{n+p,p}
\end{bmatrix}$$

The $p$ columns in $X \in \mathbb{R}^{n \times p}$ span at most $\mathbb{R}^n$ since $p > n$. The $p$ column vectors in $\tilde X$ are in $\mathbb{R}^{n + p}$. 
Let $U$ be the $\mathbb{R}^n$ space that the column vectors of $X$ are in, and $V$ be the $p$-dimensional complement of $U$, where $V = U^\bot$.
Since $\sqrt\lambda I$ is a $p \times p$ non-singular matrix that spans $V$, the $p$ column vectors in $\tilde X$ are linearly independant. 
Each column vector has one non-zero component that corresponds to a dimension in $V$. Therefore, regardless of what the rank of $X$ may be, the column rank of $\tilde X$ will always be full or equal to $p$.

**c.** Write out an explicit formula for $\hat\beta_{ridge}$ involving $X,y,\lambda$


$$\hat\beta_{ridge} = (X^TX + \lambda I)^{-1} X^T y$$

**d.** In one sentence describe the meaning of bias of an estimator in a statistical learning context.


The bias of an estimator is the deviation of the expected value of the estimator from the true value of the parameter that it is trying to estimate.

**e.** In statistics, the bias of an estimator is the difference between the estimatorâ€™s expected value and the true value of the parameter being estimated. Let $x_o\in R^p$ be a previously unseen feature vector. Prove that the prediction at $x_o$, i.e. $x_o^T \hat\beta_R$, is indeed a biased estimate of $x_oT \beta^*$, for any $\lambda > 0$. Write the bias in terms of $\lambda, X, x_o, \ and \ \beta^*$. Note that the bias depends on $\lambda$.


$$\hat y_o = x_o^T \hat\beta_{ridge} $$
$$y_o = x_o^T \beta^* + \epsilon_o $$
$$\mathbb{E}[\hat\beta_{ridge}] =  \mathbb{E}[(X^TX + \lambda I)^{-1} X^T y]$$
$$\mathbb{E}[\hat\beta_{ridge}] = (X^TX + \lambda I)^{-1} X^T \mathbb{E}[y] $$
$$\mathbb{E}[\hat\beta_{ridge}] = (X^TX + \lambda I)^{-1} X^T \mathbb{E}[X\beta^*+\epsilon] $$
$$\mathbb{E}[\hat\beta_{ridge}] = (X^TX + \lambda I)^{-1} X^T (\mathbb{E}[X\beta^*] + \mathbb{E}[\epsilon]) $$
$$\mathbb{E}[\hat\beta_{ridge}] = (X^TX + \lambda I)^{-1} X^T (X\beta^* + 0) $$
$$\mathbb{E}[\hat\beta_{ridge}] = (X^TX + \lambda I)^{-1} (X^T X) \beta^* $$

We can see that the expected value of $\hat\beta_{ridge} \neq \beta^*$ if $\lambda \neq 0$.

$$Bias(\hat y_o) = \mathbb{E}[\hat y_o - y_o] $$
$$Bias(\hat y_o) = \mathbb{E}[x_o^T \hat\beta_{ridge} - (x_o^T \beta^* + \epsilon_o)] $$
$$Bias(\hat y_o) = x_o^T \mathbb{E}[\hat\beta_{ridge}] - x_o^T \beta^* - \mathbb{E}[\epsilon_o]$$
$$Bias(\hat y_o) = x_o^T \mathbb{E}[\hat\beta_{ridge}] - x_o^T \beta^* - 0 $$
$$Bias(\hat y_o) =x_o^T (X^TX + \lambda I)^{-1} (X^T X) \beta^* - x_o^T \beta^*$$

**f.** For what value $\lambda$ is the bias zero?


$$\lambda = 0$$

When $\lambda = 0$, the the penalty term $\lambda \Vert \beta\Vert^2_2 = 0$. Therefore, the expected value $\hat\beta_{ridge}$ is equal to the true coefficient vector $\beta^*$. This means that $\hat\beta_{ridge}$ is an unbiased estimator of $\beta^*$, and has a bias of 0.

Also from part e, $Bias(x_o) =x_o^T (X^TX + \lambda I)^{-1} (X^T X) \beta^* - x_o^T \beta^*$. If we plug in $\lambda = 0$,

$$Bias(\hat y_o) =x_o^T (X^TX + 0 I)^{-1} (X^T X) \beta^* - x_o^T \beta^* $$
$$ Bias(\hat y_o) =x_o^T (X^TX)^{-1} (X^T X) \beta^* - x_o^T \beta^* $$
$$Bias(\hat y_o) =x_o^T  \beta^* - x_o^T \beta^* $$
$$Bias(\hat y_o) = 0$$

**g.** Find the covariance matrix of $\hat\beta_{ridge}$ in terms of $X, \sigma \ and \ \lambda$. For any random vector $z$, the covariance matrix is defined as $cov[z] = \mathbb{E}[zz^T] - \mathbb{E} [z] \mathbb{E} [z]^T$.

$$
cov[\hat\beta_{ridge}] = \mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] - \mathbb{E} [\hat\beta_{ridge}] \mathbb{E} [\hat\beta_{ridge}]^T \\
$$
$$
\hat\beta_{ridge} = (X^TX + \lambda I)^{-1} X^T y \\
$$
$$
\hat\beta_{ridge}^T = y^TX(X^TX+\lambda I)^{-1}\\
$$
$$
\mathbb{E}[\hat\beta_{ridge}] = (X^TX + \lambda I)^{-1} (X^T X) \beta^* \\
$$
$$
\mathbb{E}[\hat\beta_{ridge}]^T = \beta^{*T}(X^TX)(X^TX + \lambda I)^{-1}\\
$$

---

$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = \mathbb{E}[ [(X^TX + \lambda I)^{-1} X^T y][y^TX(X^TX+\lambda I)^{-1}]]\\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = \mathbb{E}[(X^TX + \lambda I)^{-1} X^T yy^TX(X^TX+\lambda I)^{-1}] \\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = (X^TX + \lambda I)^{-1} X^T \mathbb{E}[yy^T]X(X^TX+\lambda I)^{-1} \\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = (X^TX + \lambda I)^{-1} X^T \mathbb{E}[[X\beta^*+\epsilon][X\beta^*+\epsilon]^T]X(X^TX+\lambda I)^{-1} \\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = (X^TX + \lambda I)^{-1} X^T \mathbb{E}[[X\beta^*+\epsilon][\beta^{*T}X^T+\epsilon^T]]X(X^TX+\lambda I)^{-1} \\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = (X^TX + \lambda I)^{-1} X^T \mathbb{E}[X\beta^*\beta^{*T}X^T - \epsilon\beta^{*T} X^T - \epsilon^TX\beta^* +\epsilon \epsilon^T]X(X^TX+\lambda I)^{-1} \\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = (X^TX + \lambda I)^{-1} X^T [X\beta^*\beta^{*T}X^T - \mathbb{E}[\epsilon\beta^{*T} X^T ] - \mathbb{E} [\epsilon^TX\beta^*] +\mathbb{E}[\epsilon \epsilon^T]]X(X^TX+\lambda I)^{-1} \\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = (X^TX + \lambda I)^{-1} X^T [X\beta^*\beta^{*T}X^T - 0 - 0 +\sigma^2I]X(X^TX+\lambda I)^{-1} \\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = (X^TX + \lambda I)^{-1} X^T [X\beta^*\beta^{*T}X^T +\sigma^2I]X(X^TX+\lambda I)^{-1}] \\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = [(X^TX + \lambda I)^{-1} X^TX\beta^*\beta^{*T}X^T + (X^TX + \lambda I)^{-1} X^T \sigma^2I]X(X^TX+\lambda I)^{-1} \\
$$
$$
\mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] = (X^TX + \lambda I)^{-1} X^TX\beta^*\beta^{*T}X^T X(X^TX+\lambda I)^{-1}+ (X^TX + \lambda I)^{-1} X^T \sigma^2I X(X^TX+\lambda I)^{-1} \\
$$

---

$$
\mathbb{E}[\hat\beta_{ridge}] \mathbb{E}[\hat\beta_{ridge}]^T = (X^TX + \lambda I)^{-1} (X^T X) \beta^* \beta^{*T}(X^TX)(X^TX + \lambda I)^{-1} \\
$$

---

$$
cov[\hat\beta_{ridge}] = \mathbb{E}[\hat\beta_{ridge} \hat\beta_{ridge}^T] - \mathbb{E} [\hat\beta_{ridge}] \mathbb{E} [\hat\beta_{ridge}]^T \\
$$
$$
cov[\hat\beta_{ridge}] = (X^TX + \lambda I)^{-1} X^TX\beta^*\beta^{*T}X^T X(X^TX+\lambda I)^{-1}+ (X^TX + \lambda I)^{-1} X^T \sigma^2I X(X^TX+\lambda I)^{-1} 
$$
$$
- (X^TX + \lambda I)^{-1} (X^T X) \beta^* \beta^{*T}(X^TX)(X^TX + \lambda I)^{-1} \\
$$
$$
cov[\hat\beta_{ridge}] = (X^TX + \lambda I)^{-1} X^T \sigma^2I X(X^TX+\lambda I)^{-1} \\
$$
$$
cov[\hat\beta_{ridge}] = \sigma^2 (X^TX + \lambda I)^{-1} X^T X(X^TX+\lambda I)^{-1}  \\
$$


**h.** In one sentence describe the meaning of variance of an estimator in a statistical learning context.


The variance of an estimator quantifies how much the estimator is expected to vary as the dataset from the underlying data-generating process is independently resampled.

**i.** Find the variance of the of the ridge prediction at $x_o$. In other words, find the variance of $x_o^T \hat\beta_{ridge}$ in terms of $\lambda, X, x_o, \ and \ \sigma$


- $\hat\beta$ denotes the least squares estimator
$$
Var(\hat\beta) = \mathbb{E}\{[\hat\beta - \mathbb{E}(\hat\beta)] [\hat\beta - \mathbb{E}(\hat\beta)]^T\}
$$
$$
Var(\hat\beta) = \mathbb{E}\{ [(X^TX)^{-1} X^Ty - \beta^*] [(X^TX)^{-1} X^Ty - \beta^*]^T\}
$$
$$
Var(\hat\beta) = (X^TX)^{-1} X^T \mathbb{E}[yy^T] X (X^TX)^{-1} -\beta^* \beta^{*T}
$$
$$
Var(\hat\beta) = (X^TX)^{-1} X^T [X \beta^* \beta^{*T} X^T + \sigma^2 I] X (X^TX)^{-1} - \beta^* \beta^{*T}
$$
$$
Var(\hat\beta) = \beta^* \beta^{*T} + \sigma^2(X^TX)^{-1} - \beta^* \beta^{*T}
$$
$$
Var(\hat\beta) = \sigma^2(X^TX)^{-1}
$$

$$
\hat\beta_{ridge} = (X^TX + \lambda I)^{-1} X^T y\
$$
$$
\mathbb{E} [\hat\beta_{ridge}] = (X^TX + \lambda I)^{-1} X^T X \beta^*
$$
$$
Let \ W_\lambda = (X^TX + \lambda I)^{-1} X^T X
$$
$$
\mathbb{E} [\hat\beta_{ridge}] = W_\lambda \hat\beta
$$
$$Var(\hat\beta_{ridge}) =  Var(W_\lambda \hat\beta)$$
$$
Var(\hat\beta_{ridge}) = W_\lambda \ Var(\hat\beta) W_\lambda^T
$$
$$
Var(\hat\beta_{ridge}) = W_\lambda [\sigma^2 (X^TX)^{-1} ]W_\lambda^T
$$
$$
Var(\hat\beta_{ridge}) = \sigma^2 W_\lambda (X^TX)^{-1} W_\lambda^T
$$

$$
Var(\hat y_o) = Var(x_o^T \hat\beta_{ridge})
$$
$$
Var(\hat y_o) = x_o^T \ Var(\hat\beta_{ridge}) x_o
$$
$$
Var(\hat y_o) = \sigma^2 x_o^T W_\lambda (X^TX)^{-1} W_\lambda^T x_o
$$
$$
Var(\hat y_o) = \sigma^2 x_o^T (X^TX + \lambda I)^{-1} X^T X (X^TX)^{-1} (X^TX + \lambda I)^{-1} X^T X x_o
$$
$$
Var(\hat y_o) = \sigma^2 x_o^T (X^TX + \lambda I)^{-1} X^TX (X^TX + \lambda I)^{-1} x_o
$$
